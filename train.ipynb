{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1020da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "from time import sleep\n",
    "\n",
    "from FNO import FNO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff58cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global parameters\n",
    "B = 5 # batch size\n",
    "nx = 500 # npts\n",
    "Nsols = 500\n",
    "\n",
    "modes = 10\n",
    "num_fourier_layers = 4\n",
    "in_channels = 4\n",
    "out_channels = 3\n",
    "fourier_channels = 20\n",
    "activation = nn.GELU()\n",
    "\n",
    "epochs = 1\n",
    "visualize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a26f1534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./outputs_train.pkl\", 'rb') as file:\n",
    "    output_data = pkl.load(file)\n",
    "len(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e32800b",
   "metadata": {},
   "source": [
    "# Initialize Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb9258dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SodShockData(Dataset):\n",
    "    def __init__(self, path, split: str = \"train\", normalize: bool = True):\n",
    "        super(SodShockData, self).__init__()\n",
    "        with open(Path(path) / f\"inputs_{split}.pkl\", 'rb') as file:\n",
    "            self.input_data = pkl.load(file)\n",
    "\n",
    "        with open(Path(path) / f\"outputs_{split}.pkl\", 'rb') as file:\n",
    "            self.output_data = pkl.load(file)\n",
    "        self.normalize = normalize\n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, idx:int):\n",
    "        inp = self.input_data[idx]\n",
    "        x, rho, u, p = inp\n",
    "        input_data = torch.stack((torch.as_tensor(x),torch.as_tensor(rho),\n",
    "                                  torch.as_tensor(u),torch.as_tensor(p)), dim=-1)\n",
    "\n",
    "        out = self.output_data[idx]\n",
    "        rho, u, p = out\n",
    "        output_data = torch.stack((torch.as_tensor(rho),torch.as_tensor(u),\n",
    "                                   torch.as_tensor(p)), dim=-1)\n",
    "        \n",
    "        if self.normalize:\n",
    "            input_mean = input_data.mean()\n",
    "            input_std = input_data.std()\n",
    "            input_data = (input_data-input_mean)/input_std\n",
    "            \n",
    "            output_mean = output_data.mean()\n",
    "            output_std = output_data.std()\n",
    "            output_data = (output_data-output_mean)/output_std\n",
    "        \n",
    "        return (input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "405c84ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = SodShockData(\n",
    "    path=\"./\",\n",
    "    split=\"train\",\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=B,\n",
    "    shuffle=True,\n",
    ")\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f71b2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.3926, -0.0989, -0.9345],\n",
      "         [ 2.3926, -0.0989, -0.9345],\n",
      "         [ 2.3926, -0.0989, -0.9345],\n",
      "         ...,\n",
      "         [-0.8509, -0.8300, -0.9345],\n",
      "         [-0.8509, -0.8300, -0.9345],\n",
      "         [-0.8509, -0.8300, -0.9345]],\n",
      "\n",
      "        [[ 2.3536, -0.0492, -0.9387],\n",
      "         [ 2.3536, -0.0492, -0.9387],\n",
      "         [ 2.3536, -0.0492, -0.9387],\n",
      "         ...,\n",
      "         [-0.8497, -0.8275, -0.9387],\n",
      "         [-0.8497, -0.8275, -0.9387],\n",
      "         [-0.8497, -0.8275, -0.9387]],\n",
      "\n",
      "        [[ 1.6837,  0.9367, -1.0461],\n",
      "         [ 1.6837,  0.9367, -1.0461],\n",
      "         [ 1.6837,  0.9367, -1.0461],\n",
      "         ...,\n",
      "         [-0.8478, -0.7982, -1.0461],\n",
      "         [-0.8478, -0.7982, -1.0461],\n",
      "         [-0.8478, -0.7982, -1.0461]],\n",
      "\n",
      "        [[ 2.2698,  0.0633, -0.9524],\n",
      "         [ 2.2698,  0.0633, -0.9524],\n",
      "         [ 2.2698,  0.0633, -0.9524],\n",
      "         ...,\n",
      "         [-0.8508, -0.8255, -0.9524],\n",
      "         [-0.8508, -0.8255, -0.9524],\n",
      "         [-0.8508, -0.8255, -0.9524]],\n",
      "\n",
      "        [[ 1.9337,  0.5799, -1.0133],\n",
      "         [ 1.9337,  0.5799, -1.0133],\n",
      "         [ 1.9337,  0.5799, -1.0133],\n",
      "         ...,\n",
      "         [-0.8540, -0.8142, -1.0133],\n",
      "         [-0.8540, -0.8142, -1.0133],\n",
      "         [-0.8540, -0.8142, -1.0133]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    inputs, outputs = batch\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b8a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8000], Loss: 0.44120484590530396\n",
      "Epoch [21/8000], Loss: 0.21578438580036163\n",
      "Epoch [41/8000], Loss: 0.2466602325439453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m l(pred, outputs)\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(loss)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#zero_grad\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#backwards\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#step\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# print(inputs.shape, outputs.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workenv/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/workenv/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model initialization\n",
    "model = FNO(modes=modes, num_fourier_layers=num_fourier_layers, in_channels=in_channels, fourier_channels=fourier_channels, out_channels=out_channels, activation=activation)\n",
    "\n",
    "# initialize optimizer\n",
    "lr = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "l = nn.MSELoss(reduction='None')\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        inputs, outputs = batch\n",
    "        inputs = inputs.float()\n",
    "        outputs = outputs.float()\n",
    "        #print(inputs.shape)\n",
    "        #print(outputs.shape)\n",
    "        pred = model(inputs)\n",
    "        #print(pred.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = l(pred, outputs)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        # print(loss)\n",
    "        #zero_grad\n",
    "        #backwards\n",
    "        #step\n",
    "        # print(inputs.shape, outputs.shape)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e86f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros(100)\n",
    "rho = np.zeros(100)\n",
    "#torch.stack((torch.as_tensor(x), torch.as_tensor(rho)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ccf92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(name='tanh'):\n",
    "    if name in ['tanh', 'Tanh']:\n",
    "        return nn.Tanh()\n",
    "    elif name in ['relu', 'ReLU']:\n",
    "        return nn.ReLU(inplace=True)\n",
    "    elif name in ['lrelu', 'LReLU']:\n",
    "        return nn.LeakyReLU(inplace=True)\n",
    "    elif name in ['sigmoid', 'Sigmoid']:\n",
    "        return nn.Sigmoid()\n",
    "    elif name in ['softplus', 'Softplus']:\n",
    "        return nn.Softplus(beta=4)\n",
    "    elif name in ['celu', 'CeLU']:\n",
    "        return nn.CELU()\n",
    "    elif name in ['elu']:\n",
    "        return nn.ELU()\n",
    "    elif name in ['SiLU']:\n",
    "        return nn.SiLU()\n",
    "    else:\n",
    "        raise ValueError('Unknown activation function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023b11d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model initialization\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mFNO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_fourier_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_fourier_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfourier_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfourier_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/ENM_5320_Final_Project/FNO.py:24\u001b[0m, in \u001b[0;36mFNO.__init__\u001b[0;34m(self, modes, num_fourier_layers, in_channels, fourier_channels, out_channels, activation)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlifting \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_channels)\n\u001b[0;32m---> 24\u001b[0m fourier_blocks \u001b[38;5;241m=\u001b[39m [FourierBlock(modes\u001b[38;5;241m=\u001b[39mmodes, in_channels\u001b[38;5;241m=\u001b[39mfourier_channels,\n\u001b[1;32m     25\u001b[0m                                out_channels\u001b[38;5;241m=\u001b[39mfourier_channels, activation\u001b[38;5;241m=\u001b[39mactivation) \u001b[38;5;28;01mfor\u001b[39;00m __ \u001b[38;5;129;01min\u001b[39;00m num_fourier_layers]\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_blocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(fourier_blocks)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojecting \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_channels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95927ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader intialization\n",
    "\n",
    "with open(\"sod_shock_dataset.pkl\", \"rb\") as f:\n",
    "    solutions = pkl.load(f)\n",
    "\n",
    "\n",
    "# extract data\n",
    "\n",
    "# train dataset\n",
    "\n",
    "# test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db42620e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.00200401, 0.00400802, 0.00601202, 0.00801603,\n",
       "       0.01002004, 0.01202405, 0.01402806, 0.01603206, 0.01803607,\n",
       "       0.02004008, 0.02204409, 0.0240481 , 0.0260521 , 0.02805611,\n",
       "       0.03006012, 0.03206413, 0.03406814, 0.03607214, 0.03807615,\n",
       "       0.04008016, 0.04208417, 0.04408818, 0.04609218, 0.04809619,\n",
       "       0.0501002 , 0.05210421, 0.05410822, 0.05611222, 0.05811623,\n",
       "       0.06012024, 0.06212425, 0.06412826, 0.06613226, 0.06813627,\n",
       "       0.07014028, 0.07214429, 0.0741483 , 0.0761523 , 0.07815631,\n",
       "       0.08016032, 0.08216433, 0.08416834, 0.08617234, 0.08817635,\n",
       "       0.09018036, 0.09218437, 0.09418838, 0.09619238, 0.09819639,\n",
       "       0.1002004 , 0.10220441, 0.10420842, 0.10621242, 0.10821643,\n",
       "       0.11022044, 0.11222445, 0.11422846, 0.11623246, 0.11823647,\n",
       "       0.12024048, 0.12224449, 0.1242485 , 0.12625251, 0.12825651,\n",
       "       0.13026052, 0.13226453, 0.13426854, 0.13627255, 0.13827655,\n",
       "       0.14028056, 0.14228457, 0.14428858, 0.14629259, 0.14829659,\n",
       "       0.1503006 , 0.15230461, 0.15430862, 0.15631263, 0.15831663,\n",
       "       0.16032064, 0.16232465, 0.16432866, 0.16633267, 0.16833667,\n",
       "       0.17034068, 0.17234469, 0.1743487 , 0.17635271, 0.17835671,\n",
       "       0.18036072, 0.18236473, 0.18436874, 0.18637275, 0.18837675,\n",
       "       0.19038076, 0.19238477, 0.19438878, 0.19639279, 0.19839679,\n",
       "       0.2004008 , 0.20240481, 0.20440882, 0.20641283, 0.20841683,\n",
       "       0.21042084, 0.21242485, 0.21442886, 0.21643287, 0.21843687,\n",
       "       0.22044088, 0.22244489, 0.2244489 , 0.22645291, 0.22845691,\n",
       "       0.23046092, 0.23246493, 0.23446894, 0.23647295, 0.23847695,\n",
       "       0.24048096, 0.24248497, 0.24448898, 0.24649299, 0.24849699,\n",
       "       0.250501  , 0.25250501, 0.25450902, 0.25651303, 0.25851703,\n",
       "       0.26052104, 0.26252505, 0.26452906, 0.26653307, 0.26853707,\n",
       "       0.27054108, 0.27254509, 0.2745491 , 0.27655311, 0.27855711,\n",
       "       0.28056112, 0.28256513, 0.28456914, 0.28657315, 0.28857715,\n",
       "       0.29058116, 0.29258517, 0.29458918, 0.29659319, 0.29859719,\n",
       "       0.3006012 , 0.30260521, 0.30460922, 0.30661323, 0.30861723,\n",
       "       0.31062124, 0.31262525, 0.31462926, 0.31663327, 0.31863727,\n",
       "       0.32064128, 0.32264529, 0.3246493 , 0.32665331, 0.32865731,\n",
       "       0.33066132, 0.33266533, 0.33466934, 0.33667335, 0.33867735,\n",
       "       0.34068136, 0.34268537, 0.34468938, 0.34669339, 0.34869739,\n",
       "       0.3507014 , 0.35270541, 0.35470942, 0.35671343, 0.35871743,\n",
       "       0.36072144, 0.36272545, 0.36472946, 0.36673347, 0.36873747,\n",
       "       0.37074148, 0.37274549, 0.3747495 , 0.37675351, 0.37875752,\n",
       "       0.38076152, 0.38276553, 0.38476954, 0.38677355, 0.38877756,\n",
       "       0.39078156, 0.39278557, 0.39478958, 0.39679359, 0.3987976 ,\n",
       "       0.4008016 , 0.40280561, 0.40480962, 0.40681363, 0.40881764,\n",
       "       0.41082164, 0.41282565, 0.41482966, 0.41683367, 0.41883768,\n",
       "       0.42084168, 0.42284569, 0.4248497 , 0.42685371, 0.42885772,\n",
       "       0.43086172, 0.43286573, 0.43486974, 0.43687375, 0.43887776,\n",
       "       0.44088176, 0.44288577, 0.44488978, 0.44689379, 0.4488978 ,\n",
       "       0.4509018 , 0.45290581, 0.45490982, 0.45691383, 0.45891784,\n",
       "       0.46092184, 0.46292585, 0.46492986, 0.46693387, 0.46893788,\n",
       "       0.47094188, 0.47294589, 0.4749499 , 0.47695391, 0.47895792,\n",
       "       0.48096192, 0.48296593, 0.48496994, 0.48697395, 0.48897796,\n",
       "       0.49098196, 0.49298597, 0.49498998, 0.49699399, 0.498998  ,\n",
       "       0.501002  , 0.50300601, 0.50501002, 0.50701403, 0.50901804,\n",
       "       0.51102204, 0.51302605, 0.51503006, 0.51703407, 0.51903808,\n",
       "       0.52104208, 0.52304609, 0.5250501 , 0.52705411, 0.52905812,\n",
       "       0.53106212, 0.53306613, 0.53507014, 0.53707415, 0.53907816,\n",
       "       0.54108216, 0.54308617, 0.54509018, 0.54709419, 0.5490982 ,\n",
       "       0.5511022 , 0.55310621, 0.55511022, 0.55711423, 0.55911824,\n",
       "       0.56112224, 0.56312625, 0.56513026, 0.56713427, 0.56913828,\n",
       "       0.57114228, 0.57314629, 0.5751503 , 0.57715431, 0.57915832,\n",
       "       0.58116232, 0.58316633, 0.58517034, 0.58717435, 0.58917836,\n",
       "       0.59118236, 0.59318637, 0.59519038, 0.59719439, 0.5991984 ,\n",
       "       0.6012024 , 0.60320641, 0.60521042, 0.60721443, 0.60921844,\n",
       "       0.61122244, 0.61322645, 0.61523046, 0.61723447, 0.61923848,\n",
       "       0.62124248, 0.62324649, 0.6252505 , 0.62725451, 0.62925852,\n",
       "       0.63126253, 0.63326653, 0.63527054, 0.63727455, 0.63927856,\n",
       "       0.64128257, 0.64328657, 0.64529058, 0.64729459, 0.6492986 ,\n",
       "       0.65130261, 0.65330661, 0.65531062, 0.65731463, 0.65931864,\n",
       "       0.66132265, 0.66332665, 0.66533066, 0.66733467, 0.66933868,\n",
       "       0.67134269, 0.67334669, 0.6753507 , 0.67735471, 0.67935872,\n",
       "       0.68136273, 0.68336673, 0.68537074, 0.68737475, 0.68937876,\n",
       "       0.69138277, 0.69338677, 0.69539078, 0.69739479, 0.6993988 ,\n",
       "       0.70140281, 0.70340681, 0.70541082, 0.70741483, 0.70941884,\n",
       "       0.71142285, 0.71342685, 0.71543086, 0.71743487, 0.71943888,\n",
       "       0.72144289, 0.72344689, 0.7254509 , 0.72745491, 0.72945892,\n",
       "       0.73146293, 0.73346693, 0.73547094, 0.73747495, 0.73947896,\n",
       "       0.74148297, 0.74348697, 0.74549098, 0.74749499, 0.749499  ,\n",
       "       0.75150301, 0.75350701, 0.75551102, 0.75751503, 0.75951904,\n",
       "       0.76152305, 0.76352705, 0.76553106, 0.76753507, 0.76953908,\n",
       "       0.77154309, 0.77354709, 0.7755511 , 0.77755511, 0.77955912,\n",
       "       0.78156313, 0.78356713, 0.78557114, 0.78757515, 0.78957916,\n",
       "       0.79158317, 0.79358717, 0.79559118, 0.79759519, 0.7995992 ,\n",
       "       0.80160321, 0.80360721, 0.80561122, 0.80761523, 0.80961924,\n",
       "       0.81162325, 0.81362725, 0.81563126, 0.81763527, 0.81963928,\n",
       "       0.82164329, 0.82364729, 0.8256513 , 0.82765531, 0.82965932,\n",
       "       0.83166333, 0.83366733, 0.83567134, 0.83767535, 0.83967936,\n",
       "       0.84168337, 0.84368737, 0.84569138, 0.84769539, 0.8496994 ,\n",
       "       0.85170341, 0.85370741, 0.85571142, 0.85771543, 0.85971944,\n",
       "       0.86172345, 0.86372745, 0.86573146, 0.86773547, 0.86973948,\n",
       "       0.87174349, 0.87374749, 0.8757515 , 0.87775551, 0.87975952,\n",
       "       0.88176353, 0.88376754, 0.88577154, 0.88777555, 0.88977956,\n",
       "       0.89178357, 0.89378758, 0.89579158, 0.89779559, 0.8997996 ,\n",
       "       0.90180361, 0.90380762, 0.90581162, 0.90781563, 0.90981964,\n",
       "       0.91182365, 0.91382766, 0.91583166, 0.91783567, 0.91983968,\n",
       "       0.92184369, 0.9238477 , 0.9258517 , 0.92785571, 0.92985972,\n",
       "       0.93186373, 0.93386774, 0.93587174, 0.93787575, 0.93987976,\n",
       "       0.94188377, 0.94388778, 0.94589178, 0.94789579, 0.9498998 ,\n",
       "       0.95190381, 0.95390782, 0.95591182, 0.95791583, 0.95991984,\n",
       "       0.96192385, 0.96392786, 0.96593186, 0.96793587, 0.96993988,\n",
       "       0.97194389, 0.9739479 , 0.9759519 , 0.97795591, 0.97995992,\n",
       "       0.98196393, 0.98396794, 0.98597194, 0.98797595, 0.98997996,\n",
       "       0.99198397, 0.99398798, 0.99599198, 0.99799599, 1.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for (init_cond, positions, regions, values) in solutions:\n",
    "    x = values['x']\n",
    "    rho = values['rho']\n",
    "    u = values['u']\n",
    "    p = values['p']\n",
    "    break\n",
    "rho.shape\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "640e39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf318e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df54f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b95f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
